{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Linear Regression From Scratch (with Normal Equation)\n",
    "__Author__ : Mohammad Rouintan , 400222042\n",
    "\n",
    "__Course__ : Undergraduate Machine Learning Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "Implement Linear Regression using the normal equation as the training algorithm from scratch.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Normal Equation ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equation is a closed-form solution used to find the value of θ that minimizes the cost function. Another way to describe the normal equation is as a one-step algorithm used to analytically find the coefficients that minimize the loss function. Both descriptions work, but what exactly do they mean? We will start with linear regression.\n",
    "\n",
    "Linear regression makes a prediction, $\\hat{y}$, by computing the weighted sum of input features plus a bias term. Mathematically it can be represented as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\dots + \\theta_{n}x_{n}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\theta$ represents the parameters and n is the number of features.\n",
    "\n",
    "Essentially, all that occurs in the above equation is the dot product of $\\theta$, and $x$ is being summed. Thus, a more concise way to represent this is to use its vectorized form:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} &= h(\\theta) = \\theta^{T}x \\tag{2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h(\\theta)$ is the hypothesis function.\n",
    "\n",
    "Given this approximate target function, we can use our model to make predictions. To determine if our model has learned well, it’s important we measure the performance of our model on the training data. For this purpose, we compute a loss function. The goal of the training process is to find the values of theta ($\\theta$) that minimize the loss function.\n",
    "\n",
    "Here’s how we can represent our loss function mathematically:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta_{0}, \\theta_{1}, \\theta_{2}, \\dots, \\theta{m}) &= \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2 \\tag{3}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above equation, theta ($\\theta$) is a $m + 1$ dimensional vector, and our loss function is a function of the vector value. Consequently, the partial derivative of the loss function, $J$, has to be taken with respect to every parameter of $\\theta_{j}$ in turn. All of them must equal zero. Following this process and solving for all of the values of $\\theta$ from $\\theta_{0}$ to $\\theta_{m}$ will result in the values of θ that minimize the loss function.\n",
    "\n",
    "Working through the solution to the parameters $\\theta_{0}$ to $\\theta_{m}$ using the process described above results in an extremely involved derivation procedure. There is indeed a faster solution.\n",
    "Take a look at the formula for the normal equation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\theta &= (X^{T}X)^{-1}X^{T}y\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$\\theta$ → The parameters that minimize the loss function<br> \n",
    "$X$ → The input feature values for each instance<br> \n",
    "$y$ → The vector of output values for each instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for first problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each cell, you should explain your entire code. Please consider clean code in cells too and use comments if you should"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b)\n",
    "Description and code of second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for first problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each cell, you should explain your entire code. Please consider clean code in cells too and use comments if you should"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for this problem\n",
    "Write a conclusion and references which you've used in your homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
