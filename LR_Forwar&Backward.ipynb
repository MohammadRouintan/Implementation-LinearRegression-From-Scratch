{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Linear Regression with Forward & Backward Features Selection (MSE metric)\n",
    "__Author__ : Mohammad Rouintan , 400222042\n",
    "\n",
    "__Course__ : Undergraduate Machine Learning Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "Implement Forward and Backward Feature selection algorithms from scratch with MSE as the metric.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Sum Of Squares (RSS)\n",
    "$$\n",
    "\\begin{align}\n",
    "RSS &= e_{1}^{2} + e_{2}^{2} + \\dots + e_{n}^{2} \\\\\n",
    "RSS &= (y_{1} - \\hat{y}_{1})^{2} + (y_{2} - \\hat{y}_{2})^{2} + \\dots + (y_{n} - \\hat{y}_{n})^{2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Forward Selection\n",
    "Forward Selection chooses a subset of the predictor variables for the final model.\n",
    "\n",
    "We can do forward stepwise in context of linear regression whether n is less than p or n is greater than p.\n",
    "\n",
    "Forward selection is a very attractive approach, because it's both tractable and it gives a good sequence of models.\n",
    "\n",
    "1.  Start with a null model. The null model has no predictors, just one intercept (The mean over Y).\n",
    "2.  Fit p simple linear regression models, each with one of the variables in and the intercept. So basically, you just search through all the single-variable models the best one (the one that results in the lowest residual sum of squares). You pick and fix this one in the model.\n",
    "3.  Now search through the remaining p minus 1 variables and find out which variable should be added to the current model to best improve the residual sum of squares.\n",
    "4.  Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to determine the most significant variable at each step ?\n",
    "The most significant variable can be chosen so that, when added to the model:\n",
    "1. It has the smallest p-value, or\n",
    "2. It provides the highest increase in R2, or\n",
    "3. It provides the highest drop in model RSS (Residuals Sum of Squares) compared to other predictors under consideration."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose a stopping rule ?\n",
    "The stopping rule is satisfied when all remaining variables to consider have a p-value larger than some specified threshold, if added to the model.\n",
    "\n",
    "When we reach this state, forward selection will terminate and return a model that only contains variables with p-values < threshold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to determine the threshold ?\n",
    "The threshold can be:\n",
    "\n",
    "1. A fixed value (for instance: 0.05 or 0.2 or 0.5)\n",
    "2. Determined by AIC (Akaike Information Criterion)\n",
    "3. Determined by BIC (Bayesian Information Criterion)\n",
    "\n",
    "If we choose a fixed value, the threshold will be the same for all variables.\n",
    "However, if we let AIC or BIC automatically determine the threshold, it will be different for each variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does AIC determine the threshold ?\n",
    "AIC chooses the threshold according to how many degrees of freedom the variable under consideration has.\n",
    "\n",
    "Take for example the case of a binary variable (by definition it has 1 degree of freedom): According to AIC, if this variable is to be included in the model, it needs to have a p-value < 0.157.\n",
    "\n",
    "The more degrees of freedom a variable has, the lower the threshold will be."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does BIC determine the threshold ?\n",
    "BIC chooses the threshold according to the effective sample size n.\n",
    "\n",
    "For instance, for n = 20, a variable will need a p-value < 0.083 in order to enter the model.\n",
    "\n",
    "The larger n is, the lower the threshold will be.\n",
    "\n",
    "BIC is a more restrictive criterion than AIC and so yields smaller models. Therefore it is only recommended when working with large sample sizes â€” where the sample size (or number of events in case of logistic regression) exceeds 100 per independent variable [Heinze et al.].\n",
    "\n",
    "Note that both AIC (and BIC) can be applied to the pooled degrees of freedom of all unselected predictors. But applying it to individual variables (like we described above) is far more prevalent in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardSelection():\n",
    "    def __init__(self, algorithm, k_features):\n",
    "        self.algorithm = clone(algorithm)\n",
    "        self.k_features = k_features\n",
    "    \n",
    "    def calculate_mse(self, x_train, x_test, y_train, y_test, index):\n",
    "        self.algorithm.fit(x_train[:, index], y_train)\n",
    "        predicted_y = self.algorithm.predict(x_test[:, index])\n",
    "        mse = mean_squared_error(y_test, predicted_y)\n",
    "        return mse\n",
    "\n",
    "    def fit(self, x_train, x_test, y_train, y_test):\n",
    "        features = tuple(np.arange(x_train.shape[1]))\n",
    "        number_of_features = len(features)\n",
    "        self.subsets = list()\n",
    "        self.scores = list()\n",
    "        self.indices = list()\n",
    "\n",
    "        subset = list()\n",
    "        score = list()\n",
    "        for feature in combinations(features, r=1):\n",
    "            score.append(self.calculate_mse(x_train, x_test, y_train, y_test, feature))\n",
    "            subset.append(feature)\n",
    "\n",
    "        best_score = np.argmin(score)\n",
    "        self.scores.append(score[best_score])\n",
    "        self.indices = list(subset[best_score])\n",
    "        self.subsets.append(self.indices)\n",
    "\n",
    "        counter = 1\n",
    "        for i in range(1, self.k_features):\n",
    "            subset = list()\n",
    "            score = list()\n",
    "\n",
    "            for index in range(number_of_features):\n",
    "                if index not in self.indices:\n",
    "                    temp = list(self.indices)\n",
    "                    temp.append(index)\n",
    "                    score.append(self.calculate_mse(x_train, x_test, y_train, y_test, temp))\n",
    "                    subset.append(temp)\n",
    "            \n",
    "            best_score = np.argmin(score)\n",
    "            self.scores.append(score[best_score])\n",
    "            self.indices = list(subset[best_score])\n",
    "            self.subsets.append(self.indices)\n",
    "\n",
    "\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x[:, self.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(n_samples=500, n_features=10, n_informative=8, noise=15, random_state=40)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "Linear_Regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_selection = ForwardSelection(Linear_Regression, k_features=6)\n",
    "forward_selection.fit(x_train, x_test, y_train, y_test)\n",
    "x_train_fixed = forward_selection.transform(x_train)\n",
    "x_test_fixed = forward_selection.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error of the optimal model is 2038.13611\n",
      "The accuracy score of the optimal model is 0.95107\n"
     ]
    }
   ],
   "source": [
    "Linear_Regression.fit(x_train_fixed, y_train)\n",
    "predicted_y = Linear_Regression.predict(x_test_fixed)\n",
    "\n",
    "mse = mean_squared_error(predicted_y, y_test)\n",
    "accuracy = Linear_Regression.score(x_test_fixed, y_test)\n",
    "print(f\"The mean squared error of the optimal model is {mse:.5f}\")\n",
    "print(f\"The accuracy score of the optimal model is {accuracy:.5f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Elimination\n",
    "Unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.\n",
    "\n",
    "In order to be able to perform backward elimination, we need to be in a situation where we have more observations than variables because we can do least squares regression when n is greater than p. If p is greater than n, we cannot fit a least squares model. It's not even defined.\n",
    "\n",
    "1. Start with all variables in the model.\n",
    "2. Remove the variable with the largest p-value | that is, the variable that is the least statistically significant.\n",
    "3. The new (p - 1)-variable model is t, and the variable with the largest p-value is removed.\n",
    "4. Continue until a stopping rule is reached. For instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to determine the least significant variable at each step ?\n",
    "The least significant variable is a variable that:\n",
    "\n",
    "1. Has the highest p-value in the model, or\n",
    "2. Its elimination from the model causes the lowest drop in R2, or\n",
    "3. Its elimination from the model causes the lowest increase in RSS (Residuals Sum of Squares) compared to other predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose a stopping rule ?\n",
    "The stopping rule is satisfied when all remaining variables in the model have a p-value smaller than some pre-specified threshold.\n",
    "\n",
    "When we reach this state, backward elimination will terminate and return the current stepâ€™s model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to determine the threshold ?\n",
    "As with forward selection, the threshold can be:\n",
    "\n",
    "1. A fixed value (for instance: 0.05 or 0.2 or 0.5)\n",
    "2. Determined by AIC (Akaike Information Criterion)\n",
    "3. Determined by BIC (Bayesian information criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackwradElimination():\n",
    "    def __init__(self, algorithm, k_features):\n",
    "        self.algorithm = clone(algorithm)\n",
    "        self.k_features = k_features\n",
    "    \n",
    "    def calculate_mse(self, x_train, x_test, y_train, y_test, index):\n",
    "        self.algorithm.fit(x_train[:, index], y_train)\n",
    "        predicted_y = self.algorithm.predict(x_test[:, index])\n",
    "        mse = mean_squared_error(y_test, predicted_y)\n",
    "        return mse\n",
    "\n",
    "    def fit(self, x_train, x_test, y_train, y_test):\n",
    "        features = tuple(np.arange(x_train.shape[1]))\n",
    "        number_of_features = len(features)\n",
    "        self.subsets = list()\n",
    "        self.scores = list()\n",
    "        self.indices = list()\n",
    "\n",
    "        subset = list()\n",
    "        score = list()\n",
    "        for feature in combinations(features, r=1):\n",
    "            score.append(self.calculate_mse(x_train, x_test, y_train, y_test, feature))\n",
    "            subset.append(feature)\n",
    "\n",
    "        best_score = np.argmin(score)\n",
    "        self.scores.append(score[best_score])\n",
    "        self.indices = list(subset[best_score])\n",
    "        self.subsets.append(self.indices)\n",
    "\n",
    "        counter = 1\n",
    "        for i in range(1, self.k_features):\n",
    "            subset = list()\n",
    "            score = list()\n",
    "\n",
    "            for index in range(number_of_features):\n",
    "                if index not in self.indices:\n",
    "                    temp = list(self.indices)\n",
    "                    temp.append(index)\n",
    "                    score.append(self.calculate_mse(x_train, x_test, y_train, y_test, temp))\n",
    "                    subset.append(temp)\n",
    "            \n",
    "            best_score = np.argmin(score)\n",
    "            self.scores.append(score[best_score])\n",
    "            self.indices = list(subset[best_score])\n",
    "            self.subsets.append(self.indices)\n",
    "\n",
    "\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x[:, self.indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for this problem\n",
    "Write a conclusion and references which you've used in your homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
