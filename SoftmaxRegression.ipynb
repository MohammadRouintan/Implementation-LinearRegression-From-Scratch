{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Batch Gradient Descent with early stopping for Softmax Regression\n",
    "__Author__ : Mohammad Rouintan , 400222042\n",
    "\n",
    "__Course__ : Undergraduate Machine Learning Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "Implement batch gradient descent with early stopping for softmax regression from scratch. Use it on a classification task on the Penguins dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "The Logistic Regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers. This is called $Softmax Regression$, or $Multinomial Logistic Regression$.\n",
    "\n",
    "The idea is quite simple: when given an instance $x$, the Softmax Regression model first computes a score $S_{k}(x)$ for each class $k$, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores. The equation to compute $S_{k}(x)$ should look familiar, as it is just like the equation for Linear Regression prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax score for class k\n",
    "$$\n",
    "\\begin{align}\n",
    "S_{k}(x) &= (\\theta^{(k)})^{T}x \\tag{1}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each class has its own dedicated parameter vector $\\theta^{(k)}$.All these vectors are typically stored as rows in a parameter matrix $\\Theta$.\n",
    "\n",
    "Once you have computed the score of every class for the instance $x$, you can estimate the probability $\\hat{p}_{k}$ that the instance belongs to class $k$ by running the scores through the softmax function: it computes the exponential of every score, then normalizes them (dividing by the sum of all the exponentials)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax function\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{p}_{k} &= \\sigma(s(x))_{k} = \\frac{e^{S_{k}(x)}}{\\sum\\limits_{j = 1}^{K} e^{S_{j}(x)}} \\tag{2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $K$ is the number of classes.\n",
    "2. $s(x)$ is a vector containing the scores of each class for instance $x$.\n",
    "3. $\\sigma(s(x))_{k}$ is the estimated probability that the instance $x$ belongs to class $k$ given the scores of each class for that instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class with the highest estimated probability (which is simply the class with the highest score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Regression classifier prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y} = argmax \\,\\, \\sigma(s(x))_{k} = argmax \\,\\, S_{k}(x) = argmax \\,\\, ((\\theta^{(k)})^{T}x) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n",
    "The $argmax$ operator returns the value of a variable that maximizes a function. In this equation, it returns the value of $k$ that maximizes the estimated probability $\\sigma(s(x))_{k}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how the model estimates probabilities and makes predictions, let’s take a look at training. The objective is to have a model that estimates a high probability for the target class (and consequently a low probability for the other classes). Minimizing the cost function, called the cross entropy, should lead to this objective because it penalizes the model when it estimates a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy cost function\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\Theta) = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\sum\\limits_{k = 1}^{K} y_{k}^{(i)}\\log(\\hat{p}_{k}^{(i)}) \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "$y_{k}^{(i)}$ is the target probability that the $i^{th}$ instance belongs to class $k$. In general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or not.\n",
    "Notice that when there are just two classes ($\n",
    "K = 2$), this cost function is equivalent to the Logistic Regression’s cost function "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy gradient vector for class k\n",
    "$$\n",
    "\\nabla_{\\theta^{(k)}} J(\\Theta) = \\frac{1}{m} \\sum\\limits_{i = 1}^{m} (\\hat{p}_{k}^{(i)} - y_{k}^{(i)})x^{(i)} \\tag{5}\n",
    "$$\n",
    "Now you can compute the gradient vector for every class, then use Gradient Descent (or any other optimization algorithm) to find the parameter matrix $\\Theta$ that minimizes the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression():\n",
    "    def __init__(self, eta=0.01, epochs=5000, l2=0.5):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "\n",
    "    def one_hot(y, n_labels):\n",
    "        y_one_hot = np.zeros((len(y), self.n_labels))\n",
    "        y_one_hot[np.arange(len(y)), y] = 1\n",
    "        return y_one_hot\n",
    "\n",
    "    def init_param(self, weights_shape, bias_shape, dtype='float64', scale=0.01):\n",
    "        w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n",
    "        b = np.zeros(bias_shape)\n",
    "        return b.astype(dtype), w.astype(dtype)\n",
    "\n",
    "    def soft_max(logits):\n",
    "        numerator = np.exp(logits)\n",
    "        denominator = np.sum(numerator, axis=1, keepdims=True)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def cross_entropy(y_encoded, y_proba):\n",
    "        return -np.mean(np.sum(np.log(y_proba) * y_encoded), axis=1)\n",
    "\n",
    "    def compute_gradient(X, y_encoded, y_proba):\n",
    "        m = X.shape[0]\n",
    "        dw = (1 / m) * np.dot(X.T, (y_proba - y_encoded))\n",
    "        db = (1 / m) * np.sum(y_proba - y_encoded)\n",
    "        return  \n",
    "\n",
    "    def fit(self, X, y, init_params=True):\n",
    "        if init_params:\n",
    "            self.n_classes = np.max(y) + 1\n",
    "            self.n_features = X.shape[1]\n",
    "            self.bias, self.weight = self.init_param((self.n_features, self.n_classes), (self.n_classes, ))\n",
    "\n",
    "        y_encoded = self.one_hot(y, self.n_classes)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            logits = X.dot(self.weight) + self.bias\n",
    "            softmax = self.soft_max(logits)\n",
    "            cross_ent_loss = self.cross_entropy(y_encoded, softmax)\n",
    "            l2_loss = self.l2 * np.sum(np.square(self.weight))\n",
    "            loss = cross_ent_loss + l2_loss\n",
    "            dw, db = self.compute_gradient(X, y_encoded, softmax)\n",
    "\n",
    "            self.weight -= self.eta * dw\n",
    "            self.bias -= self.eta * db\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        logits = X.dot(self.weight) + self.bias\n",
    "        softmax = self.soft_max(logits)\n",
    "        return softmax\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba_predict = self.predict_proba(X)\n",
    "        return proba_predict.argmax(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each cell, you should explain your entire code. Please consider clean code in cells too and use comments if you should"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b)\n",
    "Description and code of second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for first problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each cell, you should explain your entire code. Please consider clean code in cells too and use comments if you should"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion for this problem\n",
    "Write a conclusion and references which you've used in your homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
